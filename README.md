# Local-AI
功能概述：一个本地部署的AI应用框架

**Done：**

1. 。

**Todo：**

1. 。

## 使用

### 🛠️ 构建核心工作流程

现在，模型和工具都已就位，你可以通过自然语言，指挥AI像使用“技能”一样调用这些工具，完成复杂任务。

- **🔍 综合文献检索**
  你可以对AI说：

  > “请**使用Fetch工具**，搜索一下‘过程工程领域离子液体最新研究进展’，**然后使用Filesystem工具**从我本地的`D:\\papers`文件夹里找出所有提到‘回收率’的PDF文档，把关键信息总结给我。”

- **📊 数据处理与分析**
  你可以对AI说：

  > “我这里有一份实验数据的CSV文件，路径是`data/experiment_results.csv`。**请调用Jupyter工具**，帮我计算一下A、B两组的平均值和标准差，并生成一个折线图。”

- **💾 数据存储与记忆**
  这里需要用到SeekDB。你可以在Cherry Studio中创建一个笔记或代码片段，运行类似以下的Python代码来与数据库交互。之后，AI就能基于这个知识库回答你的问题。

  ```python
  import seekdb
  
  # 连接到你的知识库（如果不存在会自动创建）
  db = seekdb("my_research.seekdb")
  
  # 摄入你总结的文献内容或处理好的数据
  db.ingest_folder("./research_summaries")
  
  # 未来，你就可以让AI“从你的知识库里找关于...的资料”
  ```

- **✍️ 文稿撰写与整合**
  这是DeepSeek和CodeLlama模型的核心能力。你可以这样指挥AI：

  > “根据我们刚刚检索到的文献和分析好的实验数据，**起草一篇论文的‘方法与材料’部分**，要包含实验设备和数据处理方法的描述。”

### 🚀 进阶技巧与问题排查

- **分步指导**：如果AI一次性无法完成复杂指令，可以尝试将它拆解成“检索->分析->总结->写作”几个步骤，一步一步地给AI下指令。
- **模型分工**：在Cherry Studio中可以为不同任务创建不同的助手。例如，创建一个主要使用`DeepSeek-R1`的助手负责文献综述和写作，另一个使用`CodeLlama`的助手专门负责数据分析和脚本编写。
- **遇到问题怎么办**：
  - **工具未启用**：检查聊天框下方的MCP工具图标是否已点亮（绿色）。
  - **模型不理解**：确保你使用了支持函数调用（工具使用）的模型。
  - **路径错误**：在涉及文件操作时，使用绝对路径通常更可靠。

## 初步开发思路

将 **Cherry Studio** 作为你整个智能助手的“指挥中心”。相比Trae插件，它能更统一、便捷地管理你的本地模型和各种工具（MCP服务器）。

### 🔧 配置Cherry Studio并连接模型与工具

**安装与模型配置**

1.  **安装Cherry Studio**：从其GitHub发布页面下载并安装Windows版本的客户端。
2.  **连接本地Ollama模型**：在Cherry Studio的设置中，添加一个本地模型服务。将API地址指向你本地的Ollama服务（通常是 `http://localhost:11434`），然后你就可以看到并添加已部署的`deepseek-r1`和`codellama`模型了。

**部署与配置MCP服务器**
MCP（模型上下文协议）服务器是赋予AI使用工具能力的关键。我为你规划了几个科研核心流程所需的MCP服务器，你可以根据下方的表格，通过Conda环境`ai_1`快速安装和配置。

| 智能体模块   | 核心功能     | 推荐MCP服务器  | 安装与简要配置                                               |
| :----------- | :----------- | :------------- | :----------------------------------------------------------- |
| **文献检索** | 网络信息抓取 | **暂无**       | 无免费的好用方案【2025.11】。                                |
| **文献检索** | 本地PDF读取  | **Filesystem** | 使用NPX方式添加`@modelcontextprotocol/server-filesystem`。在参数中授权它访问你存放文献的目录（例如`D:\\research_papers`）。 |
| **数据处理** | 运行分析代码 | **Jupyter**    | 已安装`jupyter_mcp_server`。在Cherry Studio中，需要通过**手动配置**的方式添加它：选择“STDIO”类型，命令填写你conda环境`ai_1`下的Python解释器绝对路径，参数填写`-m`, `jupyter_mcp_server`。<br>使用时需打开jupyter并运行`python -m jupyter_mcp_server`（`ai_1`环境中），且有bug未解决，暂不使用。 |
| **数据存储** | 构建知识库   | **SeekDB**     | 这是一个AI原生数据库。Windows中使用`pip install pyseekdb`安装。它是一个Python库，后续通过代码集成，而非作为MCP服务器添加到Cherry Studio。 |



完成上述配置后，记得重启Cherry Studio。在聊天界面，你应该能看到这些MCP服务器的图标，在使用前需要手动**点击开启**。



## 主要版本号

| Name | Version | Function |
| :--: | :-----: | :-----: |
| python | 3.10 |  |
| ollama | 0.4.7 |  |
| Node.js | 18.20.8 | 运行多数MCP服务器的基础 |

插件支持：Ollama